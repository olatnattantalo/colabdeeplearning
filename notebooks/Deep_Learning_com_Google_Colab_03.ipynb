{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning com Google Colab - 03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VFjvPanHwXSy1XZYbts57OTA9MgU2MpL",
      "authorship_tag": "ABX9TyOaJaURJkk4ajXZAtMu4QeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olatnattantalo/colabdeeplearning/blob/master/notebooks/Deep_Learning_com_Google_Colab_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP65moYi0hMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k44i7ou0ur5",
        "colab_type": "code",
        "outputId": "ad04fd64-27f0-454d-af69-4cd0c4bb320e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VMuec2i04Uz",
        "colab_type": "code",
        "outputId": "7b135bc8-8a69-4db1-d565-885b990b388b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device('cuda')\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcvSzwVL1UKs",
        "colab_type": "code",
        "outputId": "5fe519e7-ea0a-44c7-f167-ddc2f53b4ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "torch.rand(3, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2241, 0.1207, 0.4927],\n",
              "        [0.0066, 0.4226, 0.4362],\n",
              "        [0.7747, 0.4345, 0.1190]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhXGaLr41gAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = torch.rand(3, 3).to(device) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dQJhQ9v14yl",
        "colab_type": "code",
        "outputId": "2ce29787-2a11-4feb-f7e6-371990a4aee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3960, 0.8370, 0.7597],\n",
            "        [0.7644, 0.2984, 0.0179],\n",
            "        [0.9854, 0.8884, 0.5772]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBOjxJ4p1-sp",
        "colab_type": "code",
        "outputId": "0a524643-121c-47e8-de8b-9da9082d317c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "c = torch.rand(3, 3).cuda()\n",
        "c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6959, 0.2784, 0.6864],\n",
              "        [0.4959, 0.6705, 0.1636],\n",
              "        [0.2874, 0.3595, 0.1463]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXoOJ5AH2kWv",
        "colab_type": "code",
        "outputId": "697da44d-3bb5-4e1f-c3ef-927cfc15fc52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "a = torch.ones(400, 400)\n",
        "for _ in range(10000):\n",
        "    a += a\n",
        "duration = time.time() - start\n",
        "print(f'CPU time: {duration}')\n",
        "\n",
        "print(\"----------------------\")\n",
        "\n",
        "start = time.time()\n",
        "b = torch.ones(400, 400).to(device)\n",
        "for _ in range(10000):\n",
        "    b += b\n",
        "duration = time.time() - start\n",
        "print(f'GPU time: {duration}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU time: 0.1978588104248047\n",
            "----------------------\n",
            "GPU time: 0.07151651382446289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o1gn9Xn9U-e",
        "colab_type": "code",
        "outputId": "448cd73e-1b44-4283-e0db-6ff56f41e2ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 11.2 GB  | Proc size: 2.5 GB\n",
            "GPU RAM Free: 15563MB | Used: 717MB | Util   4% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poBdKxt294xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}